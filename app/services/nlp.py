import re
from enum import Enum
import os
from openai import AsyncOpenAI

from ..config import get_settings

settings = get_settings()

# ĞŸÑ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ xray-Ğ¸Ğ½ÑÑ‚Ğ°Ğ½Ñ, Ğ½Ğ°Ğ¿Ñ€. Ğ½Ğ° 1082)
os.environ["http_proxy"] = "socks5h://host.docker.internal:1082"
os.environ["https_proxy"] = "socks5h://host.docker.internal:1082"

class IntentLabel(str, Enum):
    accept = "accept"
    reject = "reject"
    question = "question"
    ambiguous = "ambiguous"


class LeadConversationAI:
    def __init__(self) -> None:
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
        self._model = settings.openai_model
        self._accept_patterns = [
            r"\bĞ´Ğ°\b",
            r"\bĞ´Ğ°Ğ²Ğ°Ğ¹(Ñ‚Ğµ)?\b",
            r"\bĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾\b",
            r"\bĞ³Ğ¾Ñ‚Ğ¾Ğ²\b",
            r"\bÑĞ¾Ğ³Ğ»Ğ°Ñ(ĞµĞ½|Ğ½Ğ°)\b",
            r"\bĞ¾Ğº\b",
            r"\bĞ¾ĞºĞµĞ¹\b",
            r"\bĞ¿Ğ¾ĞµÑ…Ğ°Ğ»Ğ¸\b",
            r"\bĞ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾\b",
        ]
        self._reject_patterns = [
            r"\bĞ½ĞµÑ‚\b",
            r"\bĞ½Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾\b",
            r"\bĞ½ĞµĞ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾\b",
            r"\bĞ½Ğµ Ğ½Ğ°Ğ´Ğ¾\b",
            r"\bĞ½Ğµ Ğ½ÑƒĞ¶ĞµĞ½\b",
            r"\bĞ½Ğµ Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ\b",
            r"\bĞ½Ğµ Ğ±ĞµÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ñ‚Ğµ\b",
            r"\bĞ¾ÑˆĞ¸Ğ±ĞºĞ°\b",
            r"\bĞ½Ğµ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ»[Ğ°Ğ¸]?\b",
            r"\bÑĞ¿Ğ°Ğ¼\b",
            r"\bĞ½Ğµ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾\b",
        ]
        self._question_patterns = [
            r"\bÑ‡Ñ‚Ğ¾\b",
            r"\bĞºĞ°Ğº\b",
            r"\bĞºĞ¾Ğ³Ğ´Ğ°\b",
            r"\bĞºĞ°ĞºĞ¸Ğµ\b",
            r"\bĞºĞ°ĞºĞ°Ñ\b",
            r"\bÑĞºĞ¾Ğ»ÑŒĞºĞ¾\b",
            r"\bĞ¼Ğ¾Ğ¶Ğ½Ğ¾\b",
            r"\bĞ¼Ğ¾Ğ³Ñƒ\b",
            r"\bÑ‡ĞµĞ¼\b",
            r"\bĞ³Ğ´Ğµ\b",
            r"ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€",
            r"Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ",
            r"Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ",
            r"Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ",
        ]

    async def classify(self, message: str) -> IntentLabel:
        text = (message or "").strip()
        if not text:
            return IntentLabel.ambiguous
        normalized = text.lower()

        if self._match(normalized, self._accept_patterns):
            return IntentLabel.accept
        if self._match(normalized, self._reject_patterns):
            return IntentLabel.reject
        if "?" in text or self._match(normalized, self._question_patterns):
            return IntentLabel.question

        prompt = (
            "Ğ¢Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑˆÑŒ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ GordovCode. "
            "ĞÑ†ĞµĞ½Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ´Ğ° Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ ÑÑ€Ğ»Ñ‹Ğº Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°:\n"
            "- accept: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ» Ğ·Ğ°ÑĞ²ĞºÑƒ Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ·Ğ²Ğ¾Ğ½.\n"
            "- reject: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ Ğ½Ğµ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ.\n"
            "- question: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ Ğ½Ğ°ÑˆĞµĞ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.\n"
            "- ambiguous: Ğ²ÑÑ‘ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ.\n"
            "ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼ (accept/reject/question/ambiguous).\n"
            f"Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ´Ğ°: ```{message}```"
        )

        completion = await self._client.responses.create(
            model=self._model,
            input=prompt,
            temperature=0.0,
        )
        raw = (completion.output_text or "").strip().lower()
        for label in IntentLabel:
            if label.value in raw:
                return label
        return IntentLabel.ambiguous

    async def generate_greeting(self, name: str) -> str:
        return settings.greeting_template.format(name=name, calendly_link=settings.calendly_link)

    async def generate_rejection_reply(self, name: str | None = None) -> str:
        prompt = (
            "Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ĞºĞ°Ğ· Ğ»Ğ¸Ğ´Ğ° Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸. "
            "ĞÑƒĞ¶Ğ½Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾ Ğ¸Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğ·Ğ° Ğ±ĞµÑĞ¿Ğ¾ĞºĞ¾Ğ¹ÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ğ¸Ñ‚ÑŒ Ğ·Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµĞ¼ Ğ½Ğ° ÑĞ²ÑĞ·Ğ¸, ĞµÑĞ»Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑÑ. "
            "Ğ¡Ñ‚Ğ¸Ğ»ÑŒ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹, 1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
        )
        return await self._single_text_response(prompt)

    async def answer_question(self, message: str) -> str:
        prompt = (
            "Ğ¢Ñ‹ â€” Ğ˜Ğ˜-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ GordovCode Ğ¸ Ğ²ĞµĞ´Ñ‘ÑˆÑŒ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºÑƒ Ñ Ğ»Ğ¸Ğ´Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€ÑŒ.\n"
            "Ğ¡Ğ»ĞµĞ´ÑƒĞ¹ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ:\n"
            "1) Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ Ğ² ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, "
            "Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ: \"Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾. ĞĞ¸Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ñ€Ğ°ÑˆĞ½Ğ¾Ğ³Ğ¾. Ğ’ ĞºĞ°ĞºĞ¾Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ’Ğ°Ğ¼ ÑƒĞ´Ğ¾Ğ±Ğ½ĞµĞµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ğ¾Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ² zoom?\" "
            "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ½ Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ ÑÑÑ‹Ğ»ĞºÑƒ.\n"
            "2) Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ±ĞµĞ· Ğ·Ğ²Ğ¾Ğ½ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‘Ñ‚ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, ÑĞºĞ°Ğ¶Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ÑŒ Ğ¸ Ğ² Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞµ, "
            "Ğ½Ğ¾ ÑĞ¾Ğ·Ğ²Ğ¾Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒÑÑ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ² ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€Ğµ. "
            f"Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ñ ÑÑÑ‹Ğ»ĞºĞ¾Ğ¹ ğŸ‘‰ {settings.calendly_link} Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ¹ "
            "\"Ğ¢Ğ°Ğ¼ Ğ¶Ğµ Ğ±ÑƒĞ´ĞµÑ‚ ÑÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Zoom. Ğ–Ğ´ĞµĞ¼ Ğ²Ğ°Ñ Ğ½Ğ° Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğµ!\" Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹.\n"
            "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GordovCode Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ:\n"
            f"{settings.company_profile}\n"
            "Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ´Ğ°: " + message
        )
        return await self._single_text_response(prompt)

    async def _single_text_response(self, prompt: str) -> str:
        completion = await self._client.responses.create(
            model=self._model,
            input=prompt,
            temperature=0.8,
        )
        return (completion.output_text or "").strip()

    @staticmethod
    def _match(text: str, patterns: list[str]) -> bool:
        return any(re.search(pattern, text) for pattern in patterns)
